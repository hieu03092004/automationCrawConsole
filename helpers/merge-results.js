const fs = require('fs');
const path = require('path');
const xlsx = require('xlsx');

function getInput(file) {
  if (!fs.existsSync(file)) {
    throw new Error(`File "${file}" kh√¥ng t·ªìn t·∫°i.`);
  }

  const workbook = xlsx.readFile(file);
  const sheetNames = workbook.SheetNames;
  const output = {};

  sheetNames.forEach(sheet => {
    output[sheet] = xlsx.utils.sheet_to_json(workbook.Sheets[sheet], { raw: true });
  });

  if (!output.Sitedeclaration) {
    throw new Error(`Missing "Sitedeclaration" sheet`);
  }
  if (output.Sitedeclaration.length !== 1) {
    throw new Error(`Sheet "Sitedeclaration" ph·∫£i ch·ªâ c√≥ 1 d√≤ng duy nh·∫•t`);
  }
  output.Sitedeclaration = output.Sitedeclaration[0];

  return output;
}

async function mergeResults() {
  try {
    // ƒê·ªçc metadata ƒë·ªÉ bi·∫øt c√≥ bao nhi√™u chunk
    const metadataFile = path.join(__dirname, '..', 'chunks', 'metadata.json');
    if (!fs.existsSync(metadataFile)) {
      throw new Error('Metadata file not found. Please run split-urls.js first.');
    }

    const metadata = JSON.parse(fs.readFileSync(metadataFile, 'utf8'));
    console.log(`Merging results from ${metadata.numberOfChunks} chunks...`);

    // Merge t·∫•t c·∫£ itemsObject t·ª´ c√°c chunk
    const mergedItemsObject = {};
    const chunkResults = [];
    let totalProcessedUrls = 0;
    let totalProcessingTime = 0;

    for (let i = 1; i <= metadata.numberOfChunks; i++) {
      const resultFile = path.join(__dirname, '..', 'results', `chunk-${i}-result.json`);
      
      if (fs.existsSync(resultFile)) {
        const chunkResult = JSON.parse(fs.readFileSync(resultFile, 'utf8'));
        chunkResults.push(chunkResult);
        
        // Merge itemsObject
        Object.assign(mergedItemsObject, chunkResult.itemsObject);
        
        totalProcessedUrls += chunkResult.processedUrls;
        totalProcessingTime += parseFloat(chunkResult.processingTime);
        
        console.log(`Loaded chunk ${i}: ${chunkResult.processedUrls} URLs processed in ${chunkResult.processingTime}`);
      } else {
        console.warn(`Warning: Result file for chunk ${i} not found`);
      }
    }

    // ƒê·ªçc th√¥ng tin project t·ª´ input file
    const data = getInput('input/input.xlsx');
    
    // T·∫°o report data
    const reportData = {
      projectName: data.Sitedeclaration['Project Name'],
      buildNumber: new Date().getTime(),
      totalUrls: metadata.totalUrls,
      totalProcessedUrls: totalProcessedUrls,
      totalProcessingTime: `${totalProcessingTime.toFixed(2)}s`,
      numberOfChunks: metadata.numberOfChunks,
      chunkResults: chunkResults,
      'items-object': mergedItemsObject
    };

    // L∆∞u report cu·ªëi c√πng
    const reportDir = path.join(__dirname, '..', 'report');
    if (!fs.existsSync(reportDir)) {
      fs.mkdirSync(reportDir, { recursive: true });
    }

    const reportFile = path.join(reportDir, 'project.json');
    fs.writeFileSync(reportFile, JSON.stringify(reportData, null, 2));

    console.log('\n=== MERGE COMPLETED ===');
    console.log(`Total URLs: ${metadata.totalUrls}`);
    console.log(`Processed URLs: ${totalProcessedUrls}`);
    console.log(`Total Processing Time: ${totalProcessingTime.toFixed(2)}s`);
    console.log(`Success Rate: ${((totalProcessedUrls / metadata.totalUrls) * 100).toFixed(2)}%`);
    console.log(`Report saved to: ${reportFile}`);

    // T·∫°o summary file
    const summary = {
      timestamp: new Date().toISOString(),
      projectName: data.Sitedeclaration['Project Name'],
      totalUrls: metadata.totalUrls,
      processedUrls: totalProcessedUrls,
      successRate: `${((totalProcessedUrls / metadata.totalUrls) * 100).toFixed(2)}%`,
      totalTime: `${totalProcessingTime.toFixed(2)}s`,
      chunks: chunkResults.map(chunk => ({
        chunkNumber: chunk.chunkNumber,
        processedUrls: chunk.processedUrls,
        processingTime: chunk.processingTime
      }))
    };

    const summaryFile = path.join(reportDir, 'summary.json');
    fs.writeFileSync(summaryFile, JSON.stringify(summary, null, 2));
    console.log(`Summary saved to: ${summaryFile}`);

    // Merge hashDataLogs t·ª´ t·∫•t c·∫£ c√°c chunk
    console.log('\n=== MERGING HASH DATA LOGS & REMAPPING LOGS ===');
    const { valueToKey, keyToValue } = await mergeHashDataLogsAndRemapLogs(chunkResults, mergedItemsObject);
    // Sau khi remap, ghi l·∫°i project.json v·ªõi logs ƒë√£ remap
    const remappedReportDir = path.join(__dirname, '..', 'report');
    const remappedReportFile = path.join(remappedReportDir, 'project.json');
    // ƒê·ªçc l·∫°i project.json, c·∫≠p nh·∫≠t chunkResults v√† mergedItemsObject ƒë√£ remap
    let remappedReportData = JSON.parse(fs.readFileSync(remappedReportFile, 'utf8'));
    remappedReportData['items-object'] = mergedItemsObject;
    remappedReportData.chunkResults = chunkResults;
    fs.writeFileSync(remappedReportFile, JSON.stringify(remappedReportData, null, 2));
    console.log('ƒê√£ ghi l·∫°i project.json v·ªõi logs ƒë√£ remap v·ªÅ key duy nh·∫•t.');

    // ƒê·∫£m b·∫£o t·∫•t c·∫£ keys t·ª´ project.json c√≥ trong hashDataLogs.json
    console.log('\n=== ENSURING ALL KEYS ARE PRESENT ===');
    await ensureAllKeysPresent(remappedReportData);

    return remappedReportData;

  } catch (error) {
    console.error('Error merging results:', error.message);
    throw error;
  }
}

// H√†m merge hashDataLogs t·ª´ t·∫•t c·∫£ c√°c chunk
async function mergeHashDataLogsAndRemapLogs(chunkResults, mergedItemsObject) {
  try {
    const metadataFile = path.join(__dirname, '..', 'chunks', 'metadata.json');
    const metadata = JSON.parse(fs.readFileSync(metadataFile, 'utf8'));
    
    // Map value -> key duy nh·∫•t (∆∞u ti√™n key ƒë·∫ßu ti√™n g·∫∑p)
    const valueToKey = new Map();
    // Map key -> value (t·∫°m)
    const keyToValue = new Map();

    // Duy·ªát qua t·∫•t c·∫£ c√°c chunk hashDataLogs
    for (let i = 1; i <= metadata.numberOfChunks; i++) {
      const chunkHashDataLogsPath = path.join(__dirname, '..', 'results', `chunk-${i}-hashDataLogs.json`);
      if (fs.existsSync(chunkHashDataLogsPath)) {
        try {
          const chunkHashData = JSON.parse(fs.readFileSync(chunkHashDataLogsPath, 'utf8'));
          for (const [key, value] of Object.entries(chunkHashData.hash)) {
            if (!valueToKey.has(value)) {
              valueToKey.set(value, key);
            }
            keyToValue.set(key, value);
          }
        } catch (error) {
          console.warn(`Error reading hashDataLogs from chunk ${i}:`, error.message);
        }
      }
    }
    // ƒê·ªçc c·∫£ hashDataLogs.json ch√≠nh n·∫øu c√≥
    const mainHashDataLogsPath = path.join(__dirname, '..', 'hashDataLogs.json');
    if (fs.existsSync(mainHashDataLogsPath)) {
      try {
        const mainHashData = JSON.parse(fs.readFileSync(mainHashDataLogsPath, 'utf8'));
        for (const [key, value] of Object.entries(mainHashData.hash)) {
          if (!valueToKey.has(value)) {
            valueToKey.set(value, key);
          }
          keyToValue.set(key, value);
        }
      } catch (error) {}
    }

    // Thu th·∫≠p t·∫•t c·∫£ keys t·ª´ logs ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ƒë·ªß mapping
    const allLogKeys = new Set();
    function collectLogKeys(logs) {
      if (!logs) return;
      const collect = arr => Array.isArray(arr) ? arr.forEach(key => allLogKeys.add(key)) : null;
      collect(logs.info);
      collect(logs.warn);
      collect(logs.error);
    }
    
    // Thu th·∫≠p t·ª´ mergedItemsObject
    for (const [url, browsers] of Object.entries(mergedItemsObject)) {
      for (const [browser, data] of Object.entries(browsers)) {
        if (data.logs) {
          collectLogKeys(data.logs);
        }
      }
    }
    // Thu th·∫≠p t·ª´ chunkResults
    for (const chunk of chunkResults) {
      for (const [url, browsers] of Object.entries(chunk.itemsObject)) {
        for (const [browser, data] of Object.entries(browsers)) {
          if (data.logs) {
            collectLogKeys(data.logs);
          }
        }
      }
    }

    // ƒê·∫£m b·∫£o m·ªçi key trong logs ƒë·ªÅu c√≥ value, n·∫øu kh√¥ng th√¨ t·∫°o placeholder
    for (const key of allLogKeys) {
      if (!keyToValue.has(key)) {
        // T√¨m value trong c√°c file hashDataLogs
        let found = false;
        for (let i = 1; i <= metadata.numberOfChunks; i++) {
          const chunkHashDataLogsPath = path.join(__dirname, '..', 'results', `chunk-${i}-hashDataLogs.json`);
          if (fs.existsSync(chunkHashDataLogsPath)) {
            try {
              const chunkHashData = JSON.parse(fs.readFileSync(chunkHashDataLogsPath, 'utf8'));
              if (chunkHashData.hash[key]) {
                const value = chunkHashData.hash[key];
                keyToValue.set(key, value);
                if (!valueToKey.has(value)) {
                  valueToKey.set(value, key);
                }
                found = true;
                break;
              }
            } catch (error) {}
          }
        }
        // N·∫øu v·∫´n ch∆∞a t√¨m th·∫•y, ki·ªÉm tra file ch√≠nh
        if (!found && fs.existsSync(mainHashDataLogsPath)) {
          try {
            const mainHashData = JSON.parse(fs.readFileSync(mainHashDataLogsPath, 'utf8'));
            if (mainHashData.hash[key]) {
              const value = mainHashData.hash[key];
              keyToValue.set(key, value);
              if (!valueToKey.has(value)) {
                valueToKey.set(value, key);
              }
              found = true;
            }
          } catch (error) {}
        }
        // N·∫øu v·∫´n ch∆∞a t√¨m th·∫•y, t·∫°o value placeholder
        if (!found) {
          const placeholder = `[PLACEHOLDER] Log message for key: ${key}`;
          keyToValue.set(key, placeholder);
          if (!valueToKey.has(placeholder)) {
            valueToKey.set(placeholder, key);
          }
        }
      }
    }

    // ƒê·∫£m b·∫£o valueToKey ch·ªâ c√≥ 1 key duy nh·∫•t cho m·ªói value
    // (∆∞u ti√™n key ƒë·∫ßu ti√™n g·∫∑p)
    // Remap l·∫°i c√°c logs trong mergedItemsObject v√† chunkResults
    function remapLogs(logs) {
      if (!logs) return logs;
      const remap = arr => Array.isArray(arr) ? arr.map(key => {
        const value = keyToValue.get(key);
        if (value && valueToKey.has(value)) {
          return valueToKey.get(value);
        }
        return key; // Gi·ªØ nguy√™n n·∫øu kh√¥ng t√¨m th·∫•y value
      }) : [];
      return {
        info: remap(logs.info),
        warn: remap(logs.warn),
        error: remap(logs.error)
      };
    }
    // Remap mergedItemsObject
    for (const [url, browsers] of Object.entries(mergedItemsObject)) {
      for (const [browser, data] of Object.entries(browsers)) {
        if (data.logs) {
          data.logs = remapLogs(data.logs);
        }
      }
    }
    // Remap chunkResults
    for (const chunk of chunkResults) {
      for (const [url, browsers] of Object.entries(chunk.itemsObject)) {
        for (const [browser, data] of Object.entries(browsers)) {
          if (data.logs) {
            data.logs = remapLogs(data.logs);
          }
        }
      }
    }
    // Ghi l·∫°i hashDataLogs.json ch·ªâ ch·ª©a 1 key cho m·ªói value
    const mergedHashData = { hash: {} };
    for (const [value, key] of valueToKey.entries()) {
      mergedHashData.hash[key] = value;
    }
    fs.writeFileSync(mainHashDataLogsPath, JSON.stringify(mergedHashData, null, 2));
    console.log(`\n=== HASH DATA LOGS MERGE COMPLETED (deduplicated, full coverage) ===`);
    console.log(`Total unique log messages: ${valueToKey.size}`);
    console.log(`Total log keys processed: ${allLogKeys.size}`);
    console.log(`Merged hashDataLogs saved to: ${mainHashDataLogsPath}`);
    return { valueToKey, keyToValue };
  } catch (error) {
    console.error('Error merging hashDataLogs:', error.message);
    throw error;
  }
}

// H√†m ƒë·∫£m b·∫£o t·∫•t c·∫£ keys t·ª´ project.json c√≥ trong hashDataLogs.json
async function ensureAllKeysPresent(reportData) {
  try {
    const mainHashDataLogsPath = path.join(__dirname, '..', 'hashDataLogs.json');
    
    if (!fs.existsSync(mainHashDataLogsPath)) {
      console.log('hashDataLogs.json not found, skipping key validation');
      return;
    }

    // ƒê·ªçc hashDataLogs.json hi·ªán t·∫°i
    let hashDataLogs = JSON.parse(fs.readFileSync(mainHashDataLogsPath, 'utf8'));
    const existingKeys = new Set(Object.keys(hashDataLogs.hash));
    
    // Extract t·∫•t c·∫£ keys t·ª´ project.json
    const allKeysFromProject = new Set();
    
    for (const [url, browsers] of Object.entries(reportData['items-object'])) {
      for (const [browser, data] of Object.entries(browsers)) {
        if (data.logs) {
          const logs = data.logs;
          const logKeys = [
            ...logs.info,
            ...logs.warn,
            ...logs.error
          ];
          logKeys.forEach(key => allKeysFromProject.add(key));
        }
      }
    }
    
    // T√¨m keys b·ªã thi·∫øu
    const missingKeys = Array.from(allKeysFromProject).filter(key => !existingKeys.has(key));
    
    if (missingKeys.length === 0) {
      console.log('‚úÖ All keys from project.json are present in hashDataLogs.json');
      return;
    }
    
    console.log(`‚ö†Ô∏è  Found ${missingKeys.length} missing keys, adding placeholder values...`);
    
    // Th√™m placeholder values cho missing keys
    let addedKeys = 0;
    for (const key of missingKeys) {
      const placeholderMessage = `[PLACEHOLDER] Log message for key: ${key}`;
      hashDataLogs.hash[key] = placeholderMessage;
      addedKeys++;
    }
    
    // L∆∞u file ƒë√£ c·∫≠p nh·∫≠t
    fs.writeFileSync(mainHashDataLogsPath, JSON.stringify(hashDataLogs, null, 2));
    
    console.log(`‚úÖ Added ${addedKeys} placeholder keys to hashDataLogs.json`);
    console.log(`üìä Total keys in hashDataLogs.json: ${Object.keys(hashDataLogs.hash).length}`);
    
  } catch (error) {
    console.error('Error ensuring all keys are present:', error.message);
  }
}

async function main() {
  try {
    await mergeResults();
    console.log('Results merging completed successfully!');
  } catch (error) {
    console.error('Failed to merge results:', error.message);
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = { mergeResults }; 
// test-small-parallel.js - Test v·ªõi s·ªë l∆∞·ª£ng URLs nh·ªè
const { fork, execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

async function createSmallTestData() {
  console.log('1Ô∏è‚É£ T·∫°o test data nh·ªè...');
  
  // T·∫°o file URLs nh·ªè ƒë·ªÉ test
  const testUrls = [
    'https://www.google.com',
    'https://www.github.com',
    'https://www.stackoverflow.com'
  ];
  
  fs.writeFileSync('urls.json', JSON.stringify(testUrls, null, 2));
  console.log(`‚úÖ T·∫°o ${testUrls.length} URLs test`);
  return testUrls.length;
}

async function splitUrlsForTest() {
  console.log('2Ô∏è‚É£ Splitting URLs for test...');
  
  // ƒê·ªçc URLs t·ª´ file urls.json
  const urls = JSON.parse(fs.readFileSync('urls.json', 'utf8'));
  console.log(`Total URLs found: ${urls.length}`);
  
  // T·∫°o th∆∞ m·ª•c chunks n·∫øu ch∆∞a c√≥
  const chunksDir = path.join(__dirname, 'chunks');
  if (!fs.existsSync(chunksDir)) {
    fs.mkdirSync(chunksDir, { recursive: true });
  }
  
  // Split th√†nh chunks nh·ªè (1 URL per chunk cho test)
  const chunkSize = 1;
  const numberOfChunks = Math.ceil(urls.length / chunkSize);
  
  console.log(`Splitting into ${numberOfChunks} chunks, ${chunkSize} URLs per chunk`);
  
  const chunks = [];
  for (let i = 0; i < urls.length; i += chunkSize) {
    chunks.push(urls.slice(i, i + chunkSize));
  }
  
  // L∆∞u t·ª´ng chunk v√†o file ri√™ng bi·ªát
  chunks.forEach((chunk, index) => {
    const chunkFile = path.join(chunksDir, `chunk-${index + 1}.json`);
    fs.writeFileSync(chunkFile, JSON.stringify(chunk, null, 2));
    console.log(`Created chunk-${index + 1}.json with ${chunk.length} URLs`);
  });
  
  // L∆∞u metadata v·ªÅ chunks
  const metadata = {
    totalUrls: urls.length,
    numberOfChunks: chunks.length,
    chunkSize: chunkSize,
    chunks: chunks.map((chunk, index) => ({
      chunkNumber: index + 1,
      urlCount: chunk.length,
      filename: `chunk-${index + 1}.json`
    }))
  };
  
  fs.writeFileSync(
    path.join(chunksDir, 'metadata.json'),
    JSON.stringify(metadata, null, 2)
  );
  
  console.log('URL splitting completed successfully!');
  console.log(`Metadata saved to chunks/metadata.json`);
  
  return chunks.length;
}

function runChunk(i) {
  return new Promise((resolve, reject) => {
    console.log(`‚Üí Forking chunk ${i}‚Ä¶`);
    const cp = fork(path.join(__dirname, 'helpers', 'process-chunk.js'), [String(i)], { stdio: 'inherit' });
    cp.on('exit', code => code === 0
      ? resolve(i)
      : reject(new Error(`Chunk ${i} failed (exit ${code})`))
    );
  });
}

async function mergeResults() {
  console.log('\n4Ô∏è‚É£ Testing results merging...');
  require('child_process').execSync('npm run merge', { stdio: 'inherit' });
  const summary = JSON.parse(fs.readFileSync(path.join('report','summary.json'), 'utf8'));
  console.log(`‚úÖ Merge: ${summary.processedUrls}/${summary.totalUrls} URLs (${summary.successRate})`);
  console.log(`‚è± totalTime (from summary): ${summary.totalTime}`);
}

async function main() {
  console.log('üß™ Testing Small Parallel Processing Setup...\n');
  
  // 1. T·∫°o test data nh·ªè
  const urlCount = await createSmallTestData();

  // 2. Split URLs cho test
  const chunkCount = await splitUrlsForTest();

  // 3. Process all chunks in parallel
  console.log('\n3Ô∏è‚É£ Processing chunks in PARALLEL...');
  const start = Date.now();
  await Promise.all(
    Array.from({ length: chunkCount }, (_, idx) => runChunk(idx + 1))
  );
  const end = Date.now();
  const parallelTime = ((end - start) / 1000).toFixed(2);
  console.log(`\n‚úÖ All ${chunkCount} chunks done in ${parallelTime}s (measured)`);

  // 4. Merge & final check
  await mergeResults();

  console.log('\nüéâ Small test completed!');
  console.log(`üìä Processed ${urlCount} URLs in ${parallelTime}s`);
}

main().catch(err => {
  console.error('\n‚ùå Test failed:', err.message);
  process.exit(1);
}); 